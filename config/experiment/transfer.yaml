type: transfer

experiment:
  preprocessed_file: "dump/preprocessed_data_custom.pkl"
  features_file: "dump/features.pkl"

  #"true" when you only want to prepare
  # recorder files (cluster_model.pth + base_model_cluster*.pth)
  # and *not* execute the full MTL training.
  prepare_recorder: true

  clustering:
    method: "kmeans"
    kmeans:
      n_clusters: 4
      init: "k-means++"
      max_iter: 300
      n_init: 10
      random_state: 42
    hierarchical:
      n_clusters: 4
      linkage: "ward"
    dbscan:
      eps: 1.0
      min_samples: 5
      metric: "euclidean"
      leaf_size: 30

  restrict_to_cluster: false
  cluster_id:         null

  mtl:
    backbone:
      #n_times: 500  
      n_times: 600  # new dataset
      drop_prob: 0.15
      n_filters_time: 25
      n_filters_spat: 25
      filter_time_length: 10
      pool_time_length: 3

    model:
      n_outputs: 3
      backbone:
        dropout: 0.5
        n_times: 400
      head:
        hidden_dim: 128
        dropout: 0.5

    training:
      n_runs: 1
      seed_start: 42
      epochs: 1
      batch_size: 64
      #learning_rate: [0.001, 0.005, 0.01]   
      learning_rate: [0.0001, 0.0001, 0.0001]
      lambda_bias: [1e-5, 1e-5, 1e-5]  
      optimizer:
        name: "Adam"            
        weight_decay: 0.001     
      loss: "cross_entropy"
    
    mtl_model_output: "dump/trained_models/mtl"

  transfer:
    save_pooled_only: false  # if true, only one .pth (pooled) is saved
    save_strategy: universal   # "swa" or "best_run" or "universal"
    universal_epochs: 1   # upper bound on pooled fine-tune
    early_stop_patience: 5       # stop after 5 epochs with no val-loss drop
    swa_lr:    0.00005
    swa_start: 2    
    #pretrained_mtl_model: null
    pretrained_mtl_model: dump/trained_models/mtl/mtl_weights.pth
    init_from_scratch: false 
    freeze_backbone: false
    freeze_until_layer: null  # freeze up to this layer, or null/None for none
    backbone_lr: 0.00001          # Lower LR for backbone
    head_lr: 0.001                # Higher LR for head(s)
    n_runs: 1
    seed_start: 42
    epochs: 1
    #lr: 0.001
    lr: 0.0001
    weight_decay: 0.001
    batch_size: 64

    device: "cpu"

    tl_model_output: "dump/trained_models/tl"

    model:
      n_outputs: 3
      n_clusters_pretrained: 4   # Number of clusters used during MTL pretraining.
    head_hidden_dim: 128
    head_dropout: 0.5

  evaluators:
    quantitative:
      metrics:
        - "accuracy"
        - "kappa"
        - "precision"
        - "recall"
        - "f1_score"
      cluster_metrics: ${.metrics}

    qualitative:
      visualizations:
        - "pca"
        - "tsne"
        - "confusion_matrix"
        - "roc_curve"
        - "cluster_scatter"
        - "delta_by_cluster"
        - "subject_delta_sorted"
        - "delta_violin"
      pca_n_components: 3
      tsne:
        perplexity: 30
        n_iter:     1000
    mtl_output_dir: "./evaluation_plots/mtl"
    tl_output_dir: "./evaluation_plots/tl"


